{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b2ea73-7421-48ea-9903-0f8807f7668b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the needed packages\n",
    "import os\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import oracledb\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from langchain_community.vectorstores.oraclevs import OracleVS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "from langchain_nvidia_ai_endpoints import NVIDIARerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cb0df8da-1f3a-4b7b-92f5-bd3ba0345996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the NVIDIA API key as an environment variable\n",
    "os.environ[\"NVIDIA_API_KEY\"] = \"<your nvidia key starting with nvapi**** here>\" \n",
    "# Initialize the LLM (Large Language Model) with the specified model\n",
    "llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b7510e-793a-45fa-ba4d-51ec90ea7178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chat prompt template with a system message and a user message\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", (\n",
    "        \"You are a helpful and friendly AI!\"\n",
    "        \"Your responses should be concise and no longer than two sentences.\"\n",
    "        \"Say you don't know if you don't have this information.\"\n",
    "    )),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "# Chain the prompt, LLM, and output parser together\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b480e282-bab0-4b44-b0aa-b4791590bf80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A CPU (Central Processing Unit) is the brain of your computer, handling general computing tasks, executing instructions, and performing calculations. A GPU (Graphics Processing Unit) is designed specifically for handling graphics and computationally intensive tasks, like gaming, video editing, and scientific simulations, with many cores performing parallel processing.\n"
     ]
    }
   ],
   "source": [
    "# Example questions to invoke the LLM chain\n",
    "print(chain.invoke({\"question\": \"What's the difference between a GPU and a CPU?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0082e754-cc38-4120-b19c-e8100ecbffba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with the NVIDIA H200, as it doesn't seem to be a publicly recognized product.\n"
     ]
    }
   ],
   "source": [
    "# Example questions to invoke the LLM chain\n",
    "print(chain.invoke({\"question\": \"What does the H in the NVIDIA H200 stand for?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "865decf9-1767-4f5f-b006-0386d237c0e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The database user name is: vector\n",
      "Database connection information is: localhost:1521/freepdb1\n",
      "Connection successful!\n"
     ]
    }
   ],
   "source": [
    "# Database connection setup\n",
    "username = \"<your username here>\"\n",
    "password = \"<your password here>\"\n",
    "host=\"<IP of your host here>\"\n",
    "port=\"<the port that you are using here>\"\n",
    "service_name=\"<service name here>\"\n",
    "dsn=host+\":\"+port+\"/\"+service_name\n",
    "\n",
    "print(\"The database user name is:\", username)\n",
    "print(\"Database connection information is:\", dsn)\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    conn23c = oracledb.connect(user=username, password=password, dsn=dsn)\n",
    "    print(\"Connection successful!\")\n",
    "except oracledb.DatabaseError as e:\n",
    "    error, = e.args\n",
    "    print(f\"Connection failed. Error code: {error.code}\")\n",
    "    print(f\"Error message: {error.message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "231ddf31-d58f-43b0-9d5e-fbaba97edc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf', 'page': 0}, page_content='NVIDIA H200 Tensor Core GPU\\u2002|\\u2002Datasheet\\u2002|\\u2002 1NVIDIA H200 Tensor Core GPU\\nSupercharging AI and HPC workloads.\\nHigher Performance With Larger, Faster Memory\\nThe NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\\nperformance computing (HPC) workloads with game-changing performance  \\nand memory capabilities. \\nBased on the NVIDIA Hopper™ architecture , the NVIDIA H200 is the first GPU to \\noffer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\\nthat’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU  with \\n1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \\ngenerative AI and large language models, while advancing scientific computing for \\nHPC workloads with better energy efficiency and lower total cost of ownership. \\nUnlock Insights With High-Performance LLM Inference\\nIn the ever-evolving landscape of AI, businesses rely on large language models to \\naddress a diverse range of inference needs. An AI inference  accelerator must deliver the \\nhighest throughput at the lowest TCO when deployed at scale for a massive user base. \\nThe H200 doubles inference performance compared to H100 GPUs when handling \\nlarge language models such as Llama2 70B.\\n.\\nPreliminary specifications. May be subject to change.\\nLlama2 13B: ISL 128, OSL 2K | Throughput | H100 SXM 1x GPU BS 64 | H200 SXM 1x GPU BS 128\\nGPT-3 175B: ISL 80, OSL 200 | x8 H100 SXM GPUs BS 64 | x8 H200 SXM GPUs BS 128\\nLlama2 70B: ISL 2K, OSL 128 | Throughput | H100 SXM 1x GPU BS 8 | H200 SXM 1x GPU BS 32.Key Features\\n >141GB of HBM3e GPU memory\\n >4.8TB/s of memory bandwidth\\n >4 petaFLOPS of FP8 performance\\n >2X LLM inference performance\\n >110X HPC performance\\nDatasheet')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a PDF document from a URL\n",
    "loader = PyPDFLoader(\"https://nvdam.widen.net/content/udc6mzrk7a/original/hpc-datasheet-sc23-h200-datasheet-3002446.pdf\")\n",
    "# Load the document into memory\n",
    "document = loader.load()\n",
    "document[0] # Print the first page of the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bc98e83-c610-46ad-af2e-8d47661a395d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks from the document: 16\n"
     ]
    }
   ],
   "source": [
    "# Initialize a text splitter to chunk the document into smaller pieces\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \".\", \";\", \",\", \" \", \"\"],\n",
    ")\n",
    "# Split the document into chunks\n",
    "document_chunks = text_splitter.split_documents(document)\n",
    "print(\"Number of chunks from the document:\", len(document_chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0d5abf6-a1f8-4425-8c3d-5ec7c859154a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the query to be used with the reranker\n",
    "query = \"What does the H in the NVIDIA H200 stand for?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3eee336e-52a8-4575-a915-615dec68434c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NVIDIA reranker with the specified model\n",
    "reranker = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\", base_url=\"http://localhost:8001/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecc76695-ac0a-4729-a166-0c8a598d7161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rerank the document chunks based on the query\n",
    "reranked_chunks = reranker.compress_documents(query=query,documents=document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67b01618-89b6-41f7-8903-c3b09ccc8616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance Score:16.3125, Page Content:NVIDIA H200 Tensor Core GPU | Datasheet |  1NVIDIA H200 Tensor Core GPU\n",
      "Supercharging AI and HPC workloads.\n",
      "Higher Performance With Larger, Faster Memory\n",
      "The NVIDIA H200 Tensor Core GPU supercharges generative AI and high-\n",
      "performance computing (HPC) workloads with game-changing performance  \n",
      "and memory capabilities. \n",
      "Based on the NVIDIA Hopper™ architecture , the NVIDIA H200 is the first GPU to \n",
      "offer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score:10.875, Page Content:NVIDIA H200 Tensor Core GPU | Datasheet |  3Unleashing AI Acceleration for Mainstream Enterprise Servers \n",
      "With H200 NVL\n",
      "The NVIDIA H200 NVL is the ideal choice for customers with space constraints within  \n",
      "the data center, delivering acceleration for every AI and HPC workload regardless of size. \n",
      "With a 1.5X memory increase and a 1.2X bandwidth increase over the previous generation, \n",
      "customers can fine-tune LLMs within a few hours and experience LLM inference 1.8X faster....\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score:6.79296875, Page Content:NVIDIA H200 Tensor Core GPU | Datasheet |  2Supercharge High-Performance Computing\n",
      "Memory bandwidth is crucial for HPC applications, as it enables faster data \n",
      "transfer and reduces complex processing bottlenecks. For memory-intensive \n",
      "HPC applications like simulations, scientific research, and artificial intelligence, \n",
      "the H200’s higher memory bandwidth ensures that data can be accessed and \n",
      "manipulated efficiently, leading to 110X faster time to results....\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score:4.41796875, Page Content:offer 141 gigabytes (GB) of HBM3e memory at 4.8 terabytes per second (TB/s)—\n",
      "that’s nearly double the capacity of the NVIDIA H100 Tensor Core GPU  with \n",
      "1.4X more memory bandwidth. The H200’s larger and faster memory accelerates \n",
      "generative AI and large language models, while advancing scientific computing for \n",
      "HPC workloads with better energy efficiency and lower total cost of ownership. \n",
      "Unlock Insights With High-Performance LLM Inference...\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Relevance Score:4.078125, Page Content:Certified Systems™ with 4 or 8 GPUsNVIDIA MGX™ H200 NVL partner and \n",
      "NVIDIA-Certified Systems with up to 8 GPUs\n",
      "NVIDIA AI Enterprise Add-on Included\n",
      "1. Preliminary specifications. May be subject to change. \n",
      "2. With sparsity.\n",
      "Ready to Get Started?\n",
      "To learn more about the NVIDIA H200 Tensor Core GPU,  \n",
      "visit nvidia.com/h200\n",
      "© 2024 NVIDIA Corporation and affiliates. All rights reserved. NVIDIA, the NVIDIA logo, HGX, Hopper, MGX,  NVIDIA-...\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print out the relevance score and page content for each chunk\n",
    "for chunks in reranked_chunks:\n",
    "\n",
    "    # Access the metadata of the document\n",
    "    metadata = chunks.metadata\n",
    "\n",
    "    # Get the page content\n",
    "    page_content = chunks.page_content\n",
    "    \n",
    "    # Print the relevance score if it exists in the metadata, followed by page content\n",
    "    if 'relevance_score' in metadata:\n",
    "        print(f\"Relevance Score:{metadata['relevance_score']}, Page Content:{page_content}...\")\n",
    "    print(f\"{'-' * 100}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77c1a332-dc05-44e7-ad69-bc18bb831eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/myenv_nemo/lib/python3.12/site-packages/langchain_nvidia_ai_endpoints/_common.py:486: UserWarning: Found nvidia/nv-embedqa-e5-v5 in available_models, but type is unknown and inference may fail.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the NVIDIA embeddings model\n",
    "embedding_model = NVIDIAEmbeddings(model=\"nvidia/nv-embedqa-e5-v5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e76a6f9c-02f4-4156-87de-8485ea9bf633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the document chunks in an Oracle vector store with the embeddings model\n",
    "vector_store = OracleVS.from_documents(\n",
    "    document_chunks,\n",
    "    embedding_model,\n",
    "    client=conn23c,\n",
    "    table_name=\"MY_DEM04\",\n",
    "    distance_strategy=DistanceStrategy.DOT_PRODUCT,\n",
    "    #tablespace=\"my_tablespace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "042581c7-46b4-46d3-862e-c2db278acbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the vector store into a retriever with the specified search parameters\n",
    "retriever =vector_store.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f6de851-ffe0-4352-9ecb-6e112928ddd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize the compressor with the reranker model\n",
    "compressor = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                          base_url=\"http://localhost:8001/v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bccfcd50-1d7d-490e-8309-05dfc1d6de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f0e8160-6678-4f9c-8eab-8a9d54275660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What does the H in the NVIDIA H200 stand for?',\n",
       " 'result': 'The \"H\" in the NVIDIA H200 stands for \"Hopper\". The NVIDIA H200 is based on the NVIDIA Hopper architecture, which is a specific design and technical architecture used by NVIDIA for their GPUs.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the query to be used with the retrieval QA chain\n",
    "query = \"What does the H in the NVIDIA H200 stand for?\"\n",
    "# Create a retrieval QA chain using the LLM and retriever\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
